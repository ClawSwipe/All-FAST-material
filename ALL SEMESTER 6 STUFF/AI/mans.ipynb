{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from pomegranate import *\n",
    "from hmmlearn import hmm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import DBSCAN\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,precision_score,f1_score,recall_score,confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras import datasets,layers,Sequential\n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D,Dense,Flatten,Dropout,GlobalAveragePooling2D\n",
    "from zipfile import ZipFile\n",
    "import os,glob\n",
    "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
    "from keras.models import Model\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.layers import BatchNormalization\n",
    "from zipfile import ZipFile\n",
    "import matplotlib.image as mpimg\n",
    "from google.colab.patches import cv2_imshow\n",
    "from PIL import Image\n",
    "import gym\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONDITIONAL PROB\n",
    "def calculate_probability():\n",
    "    p_plan1 = 0.30\n",
    "    p_plan2 = 0.20\n",
    "    p_plan3 = 0.50\n",
    "\n",
    "    p_defect_given_plan1 = 0.01\n",
    "    p_defect_given_plan2 = 0.03\n",
    "    p_defect_given_plan3 = 0.02\n",
    "\n",
    "    p_defect = p_plan1 * p_defect_given_plan1 + p_plan2 * p_defect_given_plan2 + p_plan3 * p_defect_given_plan3\n",
    "\n",
    "    p_plan1_given_defect = (p_plan1 * p_defect_given_plan1) / p_defect\n",
    "    p_plan2_given_defect = (p_plan2 * p_defect_given_plan2) / p_defect\n",
    "    p_plan3_given_defect = (p_plan3 * p_defect_given_plan3) / p_defect\n",
    "\n",
    "    return p_plan1_given_defect, p_plan2_given_defect, p_plan3_given_defect\n",
    "\n",
    "def most_likely_plan(probabilities):\n",
    "    max_prob = max(probabilities)\n",
    "    if probabilities.count(max_prob) == 1:\n",
    "        return probabilities.index(max_prob) + 1\n",
    "    else:\n",
    "        return [i + 1 for i, j in enumerate(probabilities) if j == max_prob]\n",
    "\n",
    "def main():\n",
    "    p_plan1_given_defect, p_plan2_given_defect, p_plan3_given_defect = calculate_probability()\n",
    "\n",
    "    print(\"Probability of plan 1 being responsible:\", p_plan1_given_defect)\n",
    "    print(\"Probability of plan 2 being responsible:\", p_plan2_given_defect)\n",
    "    print(\"Probability of plan 3 being responsible:\", p_plan3_given_defect)\n",
    "\n",
    "    probabilities = [p_plan1_given_defect, p_plan2_given_defect, p_plan3_given_defect]\n",
    "    most_likely = most_likely_plan(probabilities)\n",
    "\n",
    "    if isinstance(most_likely, list):\n",
    "        print(\"The following plan(s) is/are most likely responsible:\", most_likely)\n",
    "    else:\n",
    "        print(\"Plan\", most_likely, \"is most likely responsible.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pomegranate = 0.14.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MONTY HALL BN\n",
    "\n",
    "#monty hall\n",
    "\n",
    "import math\n",
    "from pomegranate import *\n",
    "\n",
    "guest = DiscreteDistribution({'A' : 1/3, 'B' : 1/3, 'C' : 1/3})\n",
    "prize = DiscreteDistribution({'A' : 1/3, 'B' : 1/3, 'C' : 1/3})\n",
    "\n",
    "monty =ConditionalProbabilityTable([[ 'A',\n",
    "'A', 'A', 0.0 ],\n",
    "[ 'A', 'A', 'B', 0.5 ],\n",
    "[ 'A', 'A', 'C', 0.5 ],\n",
    "[ 'A', 'B', 'A', 0.0 ],\n",
    "[ 'A', 'B', 'B', 0.0 ],\n",
    "[ 'A', 'B', 'C', 1.0 ],\n",
    "[ 'A', 'C', 'A', 0.0 ],\n",
    "[ 'A', 'C', 'B', 1.0 ],\n",
    "[ 'A', 'C', 'C', 0.0 ],\n",
    "[ 'B', 'A', 'A', 0.0 ],\n",
    "[ 'B', 'A', 'B', 0.0 ],\n",
    "[ 'B', 'A', 'C', 1.0 ],\n",
    "[ 'B', 'B', 'A', 0.5 ],\n",
    "[ 'B', 'B', 'B', 0.0 ],\n",
    "[ 'B', 'B', 'C', 0.5 ],\n",
    "[ 'B', 'C', 'A', 1.0 ],\n",
    "[ 'B', 'C', 'B', 0.0 ],\n",
    "[ 'B', 'C', 'C', 0.0 ],\n",
    "[ 'C', 'A', 'A', 0.0 ],\n",
    "[ 'C', 'A', 'B', 1.0 ],\n",
    "[ 'C', 'A', 'C', 0.0 ],\n",
    "[ 'C', 'B', 'A', 1.0 ],\n",
    "[ 'C', 'B', 'B', 0.0 ],\n",
    "[ 'C', 'B', 'C', 0.0 ],\n",
    "[ 'C', 'C', 'A', 0.5 ],\n",
    "[ 'C', 'C', 'B', 0.5 ],\n",
    "[ 'C', 'C', 'C', 0.0 ]], [guest, prize] )\n",
    "\n",
    "d1 = State(guest,name = 'guest')\n",
    "d2 = State(prize, name = 'prize')\n",
    "d3 = State(monty, name = 'Monty')\n",
    "\n",
    "network = BayesianNetwork()\n",
    "network.add_states(d1,d2,d3)\n",
    "network.add_edge(d1,d3)\n",
    "network.add_edge(d2,d3)\n",
    "network.bake()\n",
    "bel = network.predict_proba({'guest' : 'A'})\n",
    "print(bel)\n",
    "bel2 = network.predict_proba({'guest':'A', 'monty':'B'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALARM CALLLS BN\n",
    "\n",
    "from pomegranate import *\n",
    "# Define the states\n",
    "burglary = DiscreteDistribution({'True': 0.001, 'False': 0.999})\n",
    "earthquake = DiscreteDistribution({'True': 0.002, 'False': 0.998})\n",
    "alarm = ConditionalProbabilityTable(\n",
    "[['True', 'True', 'True', 0.95],\n",
    "['True', 'True', 'False', 0.05],\n",
    "['True', 'False', 'True', 0.94],\n",
    "['True', 'False', 'False', 0.06],\n",
    "['False', 'True', 'True', 0.29],\n",
    "['False', 'True', 'False', 0.71],\n",
    "['False', 'False', 'True', 0.001],\n",
    "['False', 'False', 'False', 0.999]],\n",
    "[burglary, earthquake]\n",
    ")\n",
    "david_calls = ConditionalProbabilityTable(\n",
    "[['True', 'True', 0.9],\n",
    "['True', 'False', 0.1],\n",
    "['False', 'True', 0.05],\n",
    "['False', 'False', 0.95]],\n",
    "[alarm]\n",
    ")\n",
    "sophia_calls = ConditionalProbabilityTable(\n",
    "[['True', 'True', 0.7],\n",
    "['True', 'False', 0.3],\n",
    "['False', 'True', 0.01],\n",
    "['False', 'False', 0.99]],\n",
    "[alarm]\n",
    ")\n",
    "# Define the nodes\n",
    "s1 = State(burglary, name=\"burglary\")\n",
    "s2 = State(earthquake, name=\"earthquake\")\n",
    "s3 = State(alarm, name=\"alarm\")\n",
    "s4 = State(david_calls, name=\"david_calls\")\n",
    "s5 = State(sophia_calls, name=\"sophia_calls\")\n",
    "# Create the Bayesian network\n",
    "network = BayesianNetwork(\"Burglary Alarm\")\n",
    "network.add_states(s1, s2, s3, s4, s5)\n",
    "network.add_edge(s1, s3)\n",
    "network.add_edge(s2, s3)\n",
    "network.add_edge(s3, s4)\n",
    "network.add_edge(s3, s5)\n",
    "network.bake()\n",
    "# Calculate the probability of the alarm given burglary\n",
    "prob_alarm_given_burglary = network.predict_proba({'burglary': 'False'})[2].parameters[0]['True']\n",
    "print(\"Probability of Alarm given Burglary:\", prob_alarm_given_burglary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#SUNNY RAINY HMM\n",
    "from hmmlearn import hmm\n",
    "import numpy as np\n",
    "\n",
    "mapping = {'tshirt':1,'jacket':0}\n",
    "\n",
    "observations = np.array([[0,1,1,0,0,0,1],[1,1,0,0,1,1,0]])\n",
    "\n",
    "model = hmm.MultinomialHMM(n_components=2)\n",
    "\n",
    "model.fit(observations)\n",
    "\n",
    "ans = model.predict(observations)\n",
    "\n",
    "ans = ['sunny' if val == 0 else 'rainy' for val in ans]\n",
    "\n",
    "print('Predicted')\n",
    "print(ans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RED & BLUE MARBLES\n",
    "# Define the number of red and blue marbles\n",
    "num_red_marbles = 10\n",
    "num_blue_marbles = 20\n",
    "\n",
    "# Total number of marbles\n",
    "total_marbles = num_red_marbles + num_blue_marbles\n",
    "\n",
    "# Probability of drawing a red marble\n",
    "prob_red = num_red_marbles / total_marbles\n",
    "\n",
    "# Probability of drawing a blue marble\n",
    "prob_blue = num_blue_marbles / total_marbles\n",
    "\n",
    "# Conditional probability of drawing a red marble given that the drawn marble is blue\n",
    "# Since it's not possible to draw a red marble given that the drawn marble is blue, the conditional probability is 0\n",
    "prob_red_given_blue = 0\n",
    "\n",
    "print(\"Probability of drawing a red marble given that it is blue:\", prob_red_given_blue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HEALTHY SICK HMM\n",
    "from hmmlearn import hmm\n",
    "import numpy as np\n",
    "\n",
    "startprob = np.array([0.5, 0.5])  # Initial state probabilities: healthy, sick\n",
    "\n",
    "\n",
    "transmat = np.array([[0.7, 0.3],   # Transition probabilities:\n",
    "                     [0.4, 0.6]])\n",
    "\n",
    "\n",
    "emissionprob = np.array([[0.9, 0.1],  # Emission probabilities:\n",
    "                         [0.2, 0.8]])\n",
    "\n",
    "\n",
    "# Create an instance of the HMM model\n",
    "model = hmm.MultinomialHMM(n_components=2)\n",
    "model.startprob_ = startprob\n",
    "model.transmat_ = transmat\n",
    "model.emissionprob_ = emissionprob\n",
    "\n",
    "# Define the observations\n",
    "observations = np.array([[1],  # cough\n",
    "                         [0],  # no cough\n",
    "                         [1],  # cough\n",
    "                         [0],  # no cough\n",
    "                         [1]]) # cough\n",
    "\n",
    "model.fit(observations)\n",
    "\n",
    "logprob, states = model.decode(observations)\n",
    "\n",
    "# Map state indices to their meanings\n",
    "state_map = {0: 'consult doc', 1: 'dont consult'}\n",
    "state_sequence = [state_map[state] for state in states]\n",
    "\n",
    "print(\"Observations (cough=1, no cough=0):\", observations.flatten())\n",
    "print(\"Most likely states:\", state_sequence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFECTIVE PLAN PROB \n",
    "# Define the prior probabilities of each plan being used\n",
    "prior_prob_plan1 = 0.30  # Plan 1\n",
    "prior_prob_plan2 = 0.20  # Plan 2\n",
    "prior_prob_plan3 = 0.50  # Plan 3\n",
    "\n",
    "# Define the probabilities of defects given each plan\n",
    "prob_defect_given_plan1 = 0.01  # P(D|P1)\n",
    "prob_defect_given_plan2 = 0.03  # P(D|P2)\n",
    "prob_defect_given_plan3 = 0.02  # P(D|P3)\n",
    "\n",
    "# Calculate the marginal probability of a defective product\n",
    "prob_defective_product = (\n",
    "    prior_prob_plan1 * prob_defect_given_plan1 +\n",
    "    prior_prob_plan2 * prob_defect_given_plan2 +\n",
    "    prior_prob_plan3 * prob_defect_given_plan3\n",
    ")\n",
    "\n",
    "# Apply Bayes' Theorem to calculate the posterior probabilities of each plan given a defective product\n",
    "posterior_prob_plan1 = (prior_prob_plan1 * prob_defect_given_plan1) / prob_defective_product\n",
    "posterior_prob_plan2 = (prior_prob_plan2 * prob_defect_given_plan2) / prob_defective_product\n",
    "posterior_prob_plan3 = (prior_prob_plan3 * prob_defect_given_plan3) / prob_defective_product\n",
    "\n",
    "# Determine which plan has the highest posterior probability\n",
    "most_likely_plan = None\n",
    "max_posterior_probability = 0.0\n",
    "for plan, posterior_prob in enumerate([posterior_prob_plan1, posterior_prob_plan2, posterior_prob_plan3], start=1):\n",
    "    if posterior_prob > max_posterior_probability:\n",
    "        most_likely_plan = plan\n",
    "        max_posterior_probability = posterior_prob\n",
    "\n",
    "# Print the results\n",
    "print(\"Posterior Probability of Plan 1:\", posterior_prob_plan1)\n",
    "print(\"Posterior Probability of Plan 2:\", posterior_prob_plan2)\n",
    "print(\"Posterior Probability of Plan 3:\", posterior_prob_plan3)\n",
    "print(\"Most likely plan responsible for the defective product:\", most_likely_plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HMM WITHOUT STUFF\n",
    "import numpy as np\n",
    "from hmmlearn import hmm\n",
    "\n",
    "prices = [100, 110, 105, 95, 100, 90, 85, 95, 105, 110]\n",
    "observations = np.array(prices).reshape(-1, 1)\n",
    "\n",
    "model = hmm.MultinomialHMM (n_components=2)\n",
    "\n",
    "model.fit(observations)\n",
    "\n",
    "predicted = model.predict(observations)\n",
    "\n",
    "state_map = {0:'up', 1: 'down'}\n",
    "\n",
    "ans = [state_map[state] for state in predicted]\n",
    "\n",
    "print(observations.flatten())\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HMM NORMAL \n",
    "\n",
    "from hmmlearn import hmm\n",
    "import numpy as np\n",
    "\n",
    "startprob = np.array([0.5,0.5])\n",
    "\n",
    "transmat = np.array([[0.7,0.3],[0.2,0.8]])\n",
    "\n",
    "emissionprob = np.array([[0.3,0.7],[0.5,0.5]])\n",
    "\n",
    "model = hmm.MultinomialHMM(n_components = 2)\n",
    "\n",
    "model.startprob_ = startprob\n",
    "model.transmat_ = transmat\n",
    "model.emissionprob_ = emissionprob\n",
    "\n",
    "observations = np.array([[1],[0],[1],[0],[1]])\n",
    "\n",
    "model.fit(observations)\n",
    "\n",
    "logprob, states = model.decode(observations)\n",
    "\n",
    "print(states)\n",
    "state_map = {0:'healthy', 1 :'sick'}\n",
    "\n",
    "stateseq = [state_map[state] for state in states]\n",
    "print(observations.flatten())\n",
    "print(stateseq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DECISION TREE / LOGISTIC REGRESSION / NAIVE BAYES\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = DecisionTreeClassifier() / LogisticRegression() / GaussianNB\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy (Decision Tree):\", accuracy)\n",
    "\n",
    "print(classification_report(y_test,y_pred,target_names = iris.target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LINEAR REGRESSION\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "X = np.array([[1500], [1800], [2200], [2500]]) # Input \n",
    "y = np.array([300000, 350000, 400000, 450000]) # Target\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "new_area = np.array([[2000]])\n",
    "predicted_price = model.predict(new_area)\n",
    "print(\"Predicted Price:\", predicted_price) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POLY REGRESSION\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[1], [2], [3], [4]])\n",
    "y = np.array([1, 4, 9, 16]) # Quadratic relationship (y = x^2)\n",
    "\n",
    "poly_features = PolynomialFeatures(degree=2)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "\n",
    "model_poly = LinearRegression()\n",
    "model_poly.fit(X_poly, y)\n",
    "\n",
    "X_new = np.array([[5]])\n",
    "X_new_poly = poly_features.transform(X_new)\n",
    "predicted_value = model_poly.predict(X_new_poly)\n",
    "print(\"Predicted Value (Polynomial Regression):\", predicted_value) # Output: [25.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RANDOM FOREST CLASSIFIER\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy (Random Forest):\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RANDOM FOREST REGRESSION\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[1], [2], [3], [4]])\n",
    "y = np.array([2, 3, 4, 5]) \n",
    "\n",
    "forest_regressor = RandomForestRegressor()\n",
    "forest_regressor.fit(X, y)\n",
    "\n",
    "X_new = np.array([[5]])\n",
    "predicted_value = forest_regressor.predict(X_new)\n",
    "print(\"Predicted Value (Random Forest Regression):\", predicted_value) # Output: [6.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM CLASS\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = SVC(kernel='linear')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy (SVM):\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM REGRESSION\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[1], [2], [3], [4]])\n",
    "y = np.array([2, 3, 4, 5])\n",
    "\n",
    "svm_regressor = SVR(kernel='linear')\n",
    "svm_regressor.fit(X, y)\n",
    "\n",
    "X_new = np.array([[5]])\n",
    "predicted_value = svm_regressor.predict(X_new)\n",
    "print(\"Predicted Value (SVM Regression):\", predicted_value) # Output: [6.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cryptarithmetic\n",
    "\n",
    "def cryptarithmetic():\n",
    "    x = {'B': 7, 'A': 4, 'S': 8, 'E': 3, 'L': 5, 'G': 1, 'M': 9}\n",
    "    print(\"BASE + BALL = GAMES\")\n",
    "    B = x['B']\n",
    "    A = x['A']\n",
    "    S = x['S']\n",
    "    E = x['E']\n",
    "    L = x['L']\n",
    "    G = x['G']\n",
    "    M = x['M']\n",
    "    print(\"Given values:\")\n",
    "    print(\"B =\", B)\n",
    "    print(\"A =\", A)\n",
    "    print(\"S =\", S)\n",
    "    print(\"E =\", E)\n",
    "    print(\"L =\", L)\n",
    "    print(\"G =\", G)\n",
    "    print(\"M =\", M)\n",
    "    print(\"\\nSolving:\")\n",
    "    print(\" \", B, A, S, E)\n",
    "    print(\"+\", B, A, L, L)\n",
    "    print(\"----------\")\n",
    "    print(\" \", M, A, G, E, S)\n",
    "\n",
    "    if (10 * B + A + 10 * B + A + 1) == (10000 * M + 1000 * A + 100 * G + 10 * E + S):\n",
    "        print(\"\\nSolution is incorrect!\")\n",
    "    else:\n",
    "        print(\"\\nSolution is correct!\")\n",
    "\n",
    "cryptarithmetic()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HMM HIDDEN STATES\n",
    "\n",
    "import numpy as np\n",
    "from hmmlearn import hmm\n",
    "\n",
    "states = [\"Normal\", \"Faculty\"]\n",
    "n_states = len(states)\n",
    "\n",
    "print('Number of hidden states :',n_states)\n",
    "observations = [\"Normal Vibrations\", \"Abnormal Vibrations\"]\n",
    "n_observations = len(observations)\n",
    "\n",
    "print('Number of observations :',n_observations)\n",
    "\n",
    "state_probability = np.array([0.5, 0.5])\n",
    "print(\"State probability: \", state_probability)\n",
    "\n",
    "transition_probability = np.array([[0.8, 0.2], [0.2, 0.8]])\n",
    "print(\"\\nTransition probability:\\n\", transition_probability)\n",
    "\n",
    "emission_probability= np.array([[0.9, 0.1], [0.2, 0.8]])\n",
    "print(\"\\nEmission probability:\\n\", emission_probability)\n",
    "model = hmm.CategoricalHMM(n_components=n_states)\n",
    "\n",
    "model.startprob_ = state_probability\n",
    "model.transmat_ = transition_probability\n",
    "\n",
    "model.emissionprob_ = emission_probability\n",
    "observations_sequence = np.array([0, 1, 0, 1, 0, 0]).reshape(-1, 1)\n",
    "hidden_states = model.predict(observations_sequence)\n",
    "\n",
    "print(\"Most likely hidden states:\", hidden_states)\n",
    "log_probability, hidden_states = model.decode(observations_sequence, lengths =\n",
    "len(observations_sequence), algorithm ='viterbi' )\n",
    "print('Log Probability :',log_probability)\n",
    "\n",
    "print(\"Most likely hidden states:\", hidden_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GAUSSIAN HMM\n",
    "\n",
    "import numpy as np\n",
    "from hmmlearn import hmm\n",
    "\n",
    "market_states = [\"High\", \"Low\"]\n",
    "n_states = len(market_states)\n",
    "closing_prices = [100, 110, 105, 95, 100, 90, 85, 95, 105, 110]\n",
    "n_obs = len(closing_prices)\n",
    "price_changes = np.diff(closing_prices)\n",
    "state_prob = np.array([0.5, 0.5])\n",
    "\n",
    "transition_prob = np.array([[0.9, 0.1],\n",
    "                            [0.1, 0.9]])\n",
    "\n",
    "model = hmm.GaussianHMM(n_components=n_states)\n",
    "\n",
    "model.startprob_ = state_prob\n",
    "model.transmat_ = transition_prob\n",
    "model.fit(price_changes.reshape(-1, 1))\n",
    "predicted_trends = [market_states[state] for state in hidden_states]\n",
    "\n",
    "print(\"Predicted Market Trends:\", predicted_trends)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CATEGORICAL HMM\n",
    "\n",
    "import numpy as np\n",
    "from hmmlearn import hmm\n",
    "\n",
    "states = [\"Normal\", \"Faculty\"]\n",
    "n_states = len(states)\n",
    "\n",
    "print('Number of hidden states :',n_states)\n",
    "observations = [\"Normal Vibrations\", \"Abnormal Vibrations\"]\n",
    "n_observations = len(observations)\n",
    "\n",
    "print('Number of observations :',n_observations)\n",
    "\n",
    "state_probability = np.array([0.5, 0.5])\n",
    "print(\"State probability: \", state_probability)\n",
    "\n",
    "transition_probability = np.array([[0.8, 0.2], [0.2, 0.8]])\n",
    "print(\"\\nTransition probability:\\n\", transition_probability)\n",
    "\n",
    "emission_probability= np.array([[0.9, 0.1], [0.2, 0.8]])\n",
    "print(\"\\nEmission probability:\\n\", emission_probability)\n",
    "model = hmm.CategoricalHMM(n_components=n_states)\n",
    "\n",
    "model.startprob_ = state_probability\n",
    "model.transmat_ = transition_probability\n",
    "\n",
    "model.emissionprob_ = emission_probability\n",
    "hidden_states = model.predict(observations_sequence)\n",
    "\n",
    "print(\"Most likely hidden states:\", hidden_states)\n",
    "log_probability, hidden_states = model.decode(observations_sequence, lengths =\n",
    "len(observations_sequence), algorithm ='viterbi' )\n",
    "print('Log Probability :',log_probability)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DECISION TREE, RANDOMFOREST, GRADIENTBOOSTING CLASSIFICATION\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "tree_model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "tree_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_tree = tree_model.predict(X_test)\n",
    "accuracy_tree = accuracy_score(y_test, y_pred_tree)\n",
    "f1_tree = f1_score(y_test, y_pred_tree)\n",
    "\n",
    "print(\"Decision Tree - Accuracy:\", accuracy_tree)\n",
    "print(\"Decision Tree - F1 Score:\", f1_tree)\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "f1_rf = f1_score(y_test, y_pred_rf)\n",
    "\n",
    "y_pred_gb = gb_model.predict(X_test)\n",
    "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
    "f1_gb = f1_score(y_test, y_pred_gb)\n",
    "\n",
    "print(\"Random Forest - Accuracy:\", accuracy_rf)\n",
    "print(\"Random Forest - F1 Score:\", f1_rf)\n",
    "\n",
    "print(\"Gradient Boosting - Accuracy:\", accuracy_gb)\n",
    "print(\"Gradient Boosting - F1 Score:\", f1_gb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper parameter tuning\n",
    "\n",
    "#Task 1\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "svm = SVC()\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],        \n",
    "    'gamma': [1, 0.1, 0.01, 0.001],\n",
    "    'kernel': ['linear', 'rbf']   \n",
    "}\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-validation Score:\", grid_search.best_score_)\n",
    "\n",
    "best_svm = grid_search.best_estimator_\n",
    "y_pred = best_svm.predict(X_test)\n",
    "\n",
    "default_svm = SVC()\n",
    "default_svm.fit(X_train, y_train)\n",
    "y_pred_default = default_svm.predict(X_test)\n",
    "\n",
    "accuracy_tuned = accuracy_score(y_test, y_pred)\n",
    "accuracy_default = accuracy_score(y_test, y_pred_default)\n",
    "\n",
    "print(\"Tuned SVM Accuracy:\", accuracy_tuned)\n",
    "print(\"Default SVM Accuracy:\", accuracy_default)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K means clustering\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "data = load_iris()\n",
    "X = data.data  # Feature matrix\n",
    "y = data.target  # Target labels\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title('K-means Clustering of Iris Dataset')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend(title='Cluster', loc='upper right')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "conf_matrix = confusion_matrix(y, clusters)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=data.target_names, yticklabels=data.target_names)\n",
    "plt.title('Confusion Matrix: Clusters vs. Actual Species')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 4\n",
    "#dimension reduction\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Load the Wine dataset\n",
    "data = load_wine()\n",
    "X = data.data  # Feature matrix\n",
    "y = data.target  # Target labels\n",
    "\n",
    "# Standardize the features (recommended for PCA and t-SNE)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Visualize the reduced-dimensional data using PCA\n",
    "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=y, palette='viridis', s=100, alpha=0.8)\n",
    "plt.title('PCA Visualization of Wine Dataset')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend(title='Wine Class', loc='upper right')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "\n",
    "env = gym.make('Taxi-v3')\n",
    "\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "Q = np.zeros((num_states, num_actions))\n",
    "print(Q)\n",
    "\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.99\n",
    "num_episodes = 10000\n",
    "max_steps_per_episode = 100\n",
    "\n",
    "epsilon = 1.0  # Exploration rate\n",
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.01\n",
    "decay_rate = 0.01\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()  # Reset the environment to start a new episode\n",
    "    done = False  # Flag indicating whether the episode is finished\n",
    "\n",
    "    for step in range(max_steps_per_episode):\n",
    "        # Exploration-exploitation trade-off\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample()  # Explore: choose a random action\n",
    "        else:\n",
    "            action = np.argmax(Q[state, :])  # Exploit: choose the action with the highest Q-value\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        Q[state, action] = Q[state, action] + learning_rate * (\n",
    "            reward + discount_factor * np.max(Q[next_state, :]) - Q[state, action]\n",
    "        )\n",
    "\n",
    "        state = next_state  # Move to the next state\n",
    "\n",
    "        if done:  # Episode is finished\n",
    "            break\n",
    "\n",
    "    # Decay exploration rate\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)\n",
    "# Evaluate the learned Q-table\n",
    "total_rewards = 0\n",
    "num_episodes_eval = 1000\n",
    "\n",
    "for episode in range(num_episodes_eval):\n",
    "    state = env.reset()  # Reset the environment to start a new episode\n",
    "    done = False  # Flag indicating whether the episode is finished\n",
    "\n",
    "    for step in range(max_steps_per_episode):\n",
    "        action = np.argmax(Q[state, :])  # Choose the action with the highest Q-value\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action)  # Take the chosen action\n",
    "\n",
    "        state = next_state  # Move to the next state\n",
    "\n",
    "        if done:  # Episode is finished\n",
    "            break\n",
    "\n",
    "\n",
    "avg_reward = total_rewards / num_episodes_eval\n",
    "print(\"Average reward:\", avg_reward)\n",
    "\n",
    "# Close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#episodes vs avg reward\n",
    "state_size = 16\n",
    "action_space = env.action_space.n\n",
    "alpha = 0.5\n",
    "gamma = 0.7\n",
    "state_action_vals = np.random.randn(state_size, action_space)\n",
    "policy = np.zeros(state_size, dtype=int)\n",
    "episodes = 20000\n",
    "eps = 1\n",
    "test_episodes = 50\n",
    "test_every = 1000\n",
    "test_episode = []\n",
    "rewards = []\n",
    "\n",
    "\n",
    "def select_action(state, eps):\n",
    "    sample = np.random.uniform()\n",
    "    if sample < eps:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        return state_action_vals[state].argmax()\n",
    "\n",
    "for ep in range(episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_action = select_action(next_state, eps)\n",
    "\n",
    "        action_value = Q[state, action]\n",
    "        next_action_value = Q[next_state, next_action]\n",
    "        delta = reward + gamma * next_action_value - action_value\n",
    "        Q[state, action] += alpha * delta\n",
    "        state, action = next_state, next_action\n",
    "\n",
    "    if ep % test_every == 0:\n",
    "        total_rewards = 0\n",
    "        for _ in range(test_episodes):\n",
    "            done = False\n",
    "            state = env.reset()\n",
    "            while not done:\n",
    "                action = np.argmax(Q[state])\n",
    "                state, reward, done, _ = env.step(action)\n",
    "                total_rewards += reward\n",
    "        rewards.append(total_rewards / test_episodes)\n",
    "        test_episode.append(ep)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(test_episode, rewards)\n",
    "ax.set_xlabel('Episode')\n",
    "_ = ax.set_ylabel('Average reward')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#smart taxi\n",
    "\n",
    "state_size = 16\n",
    "action_space = env.action_space.n\n",
    "alpha = 0.5\n",
    "gamma = 0.7\n",
    "state_action_vals = np.random.randn(state_size, action_space)\n",
    "policy = np.zeros(state_size, dtype=int)\n",
    "episodes = 20000\n",
    "eps = 1\n",
    "test_episodes = 50\n",
    "test_every = 1000\n",
    "test_episode = []\n",
    "rewards = []\n",
    "\n",
    "\n",
    "def select_action(state, eps):\n",
    "    sample = np.random.uniform()\n",
    "    if sample < eps:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        return state_action_vals[state].argmax()\n",
    "\n",
    "for ep in range(episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_action = select_action(next_state, eps)\n",
    "\n",
    "        action_value = Q[state, action]\n",
    "        next_action_value = Q[next_state, next_action]\n",
    "        delta = reward + gamma * next_action_value - action_value\n",
    "        Q[state, action] += alpha * delta\n",
    "        state, action = next_state, next_action\n",
    "\n",
    "    if ep % test_every == 0:\n",
    "        total_rewards = 0\n",
    "        for _ in range(test_episodes):\n",
    "            done = False\n",
    "            state = env.reset()\n",
    "            while not done:\n",
    "                action = np.argmax(Q[state])\n",
    "                state, reward, done, _ = env.step(action)\n",
    "                total_rewards += reward\n",
    "        rewards.append(total_rewards / test_episodes)\n",
    "        test_episode.append(ep)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(test_episode, rewards)\n",
    "ax.set_xlabel('Episode')\n",
    "_ = ax.set_ylabel('Average reward')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#increase lifetime RL\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "num_devices = 10  # Number of AI devices\n",
    "num_actions = 2  # Number of possible actions (transmit directly or forward)\n",
    "\n",
    "energy_levels = np.ones(num_devices)  # Initial energy levels of devices\n",
    "\n",
    "def step(state, action):\n",
    "    # Perform action and update energy levels\n",
    "    # Calculate reward based on energy consumption and data transmission quality\n",
    "    next_state = get_next_state(state, action)\n",
    "    reward = calculate_reward(state, action, next_state)\n",
    "    # Update energy levels based on actions taken\n",
    "    update_energy_levels(action)\n",
    "    return next_state, reward\n",
    "\n",
    "def calculate_reward(state, action, next_state):\n",
    "    # Example: Calculate reward based on the distance between current state and next state\n",
    "    # Here, we can use a simple reward function where the reward is inversely proportional to the distance\n",
    "    distance = abs(next_state - state)\n",
    "    if distance == 0:\n",
    "        # If the next state is the same as the current state, return a negative reward\n",
    "        return -1\n",
    "    else:\n",
    "        # Otherwise, return a positive reward inversely proportional to the distance\n",
    "        return 1 / distance\n",
    "\n",
    "def update_energy_levels(action):\n",
    "    # Update energy levels based on actions taken\n",
    "    global energy_levels\n",
    "    # Placeholder implementation for now\n",
    "    if action == 0:  # Transmit directly\n",
    "        energy_levels -= 0.1  # Example: Decrease energy levels for all devices when transmitting directly\n",
    "    elif action == 1:  # Forward to the next device\n",
    "        energy_levels[0] -= 0.1  # Example: Decrease energy level of the first device when forwarding\n",
    "    else:\n",
    "        raise ValueError(\"Invalid action\")\n",
    "\n",
    "def get_next_state(state, action):\n",
    "    # Determine next state based on current state and action taken\n",
    "    # For simplicity, let's assume the next state is determined by the action\n",
    "    # This can be replaced with more complex logic based on your problem domain\n",
    "    if action == 0:  # Transmit directly\n",
    "        return state\n",
    "    elif action == 1:  # Forward to the next device\n",
    "        return (state + 1) % num_devices  # Wrap around to the first device if reaching the last one\n",
    "    else:\n",
    "        raise ValueError(\"Invalid action\")\n",
    "\n",
    "# Define the reinforcement learning algorithm\n",
    "\n",
    "Q = np.zeros((num_devices, num_actions))  # Q-table\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "epsilon = 0.1  # Epsilon for epsilon-greedy policy\n",
    "\n",
    "def select_action(state):\n",
    "    # Epsilon-greedy action selection\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.choice(num_actions)  # Explore\n",
    "    else:\n",
    "        # Exploit: Choose action with the highest Q-value for the current state\n",
    "        max_Q_value = float(\"-inf\")\n",
    "        best_action = None\n",
    "        for a in range(num_actions):\n",
    "            if Q[state, a] > max_Q_value:\n",
    "                max_Q_value = Q[state, a]\n",
    "                best_action = a\n",
    "        return best_action\n",
    "\n",
    "def update_Q(state, action, reward, next_state):\n",
    "    if state is None or reward is None:\n",
    "        return  # Skip update if state or reward is None\n",
    "\n",
    "    # Ensure that state is within the valid range\n",
    "    if 0 <= state < num_devices:\n",
    "        # Q-learning update rule\n",
    "        Q[state, action] += alpha * (reward + gamma * np.max(Q[next_state, :]) - Q[state, action])\n",
    "    else:\n",
    "        # Handle the case when state is out of bounds\n",
    "        print(\"Invalid state:\", state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apriori\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "import pandas as pd\n",
    "\n",
    "dataset = [['bread', 'milk', 'eggs'],\n",
    "['bread', 'butter'],\n",
    "['milk', 'butter'],\n",
    "['bread', 'milk', 'butter'],\n",
    "['bread', 'milk']]\n",
    "\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(dataset).transform(dataset)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "frequent_itemsets = apriori(df, min_support=0.4, use_colnames=True)\n",
    "print(frequent_itemsets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DENSITY BASED CLUSTER\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "labels = dbscan.fit_predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n",
    "plt.title(\"DBSCAN Clustering\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FP ASSOCIATE\n",
    "\n",
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "import pandas as pd\n",
    "\n",
    "dataset = [['bread', 'milk', 'eggs'],\n",
    "['bread', 'butter'],\n",
    "['milk', 'butter'],\n",
    "['bread', 'milk', 'butter'],\n",
    "['bread', 'milk']]\n",
    "\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(dataset).transform(dataset)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "frequent_itemsets = fpgrowth(df, min_support=0.4, use_colnames=True)\n",
    "print(frequent_itemsets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GAUSSIAN MIXTURE \n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
    "\n",
    "gmm = GaussianMixture(n_components=4)\n",
    "labels = gmm.fit_predict(X)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n",
    "plt.title(\"Gaussian Mixture Model (GMM) Clustering\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HEIRARCHAL CLUSTERING\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60,random_state=0)\n",
    "\n",
    "agg_clustering = AgglomerativeClustering(n_clusters=4)\n",
    "labels = agg_clustering.fit_predict(X)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n",
    "plt.title(\"Hierarchical Clustering\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KMEANS CLUSTERING\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
    "\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "labels = kmeans.fit_predict(X)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=200, c='red', marker='X', label='Centroids')\n",
    "plt.title(\"K-means Clustering\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA UNSUPERVISED CLUSTERING\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')\n",
    "plt.title(\"PCA Visualization of Iris Dataset\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#QTABLE\n",
    "\n",
    "#QTABLE\n",
    "\n",
    "pip install gym\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "\n",
    "env = gym.make('Taxi-v3')\n",
    "\n",
    "numofstates = env.observation_space.n\n",
    "numofactions = env.action_space.n\n",
    "\n",
    "qtable = np.zeros((numofstates,numofactions))\n",
    "\n",
    "total_episodes = 20000       # Total episodes\n",
    "learning_rate = 0.7          # Learning rate\n",
    "max_steps = 99               # Max steps per episode\n",
    "gamma = 0.7                 # Discounting rate\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 1.0                 # Exploration rate\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.01            # Minimum exploration probability\n",
    "decay_rate = 0.1            # Exponential decay rate for exploration prob\n",
    "\n",
    "for eps in range(total_episodes):\n",
    "  state = env.reset()\n",
    "  done= False\n",
    "\n",
    "\n",
    "  for step in range(max_steps):\n",
    "    if np.random.uniform(0,1) < epsilon:\n",
    "      action = env.action_space.sample()\n",
    "    else:\n",
    "      action = np.argmax(qtable[state,:])\n",
    "\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "    qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[next_state, :]) - qtable[state, action])\n",
    "\n",
    "    state = next_state\n",
    "\n",
    "    if done:\n",
    "      break\n",
    "\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*eps)\n",
    "\n",
    "total_rewards = 0\n",
    "num_episodes_eval = 1000\n",
    "for episode in range(num_episodes_eval):\n",
    "  state = env.reset() # Reset the environment to start a new episode\n",
    "  done = False # Flag indicating whether the episode is finished\n",
    "  for step in range(max_steps):\n",
    "      action = np.argmax(qtable[state, :])\n",
    "      next_state, reward, done, _ = env.step(action) # Take the chosen action\n",
    "      total_rewards += reward # Accumulate the total reward\n",
    "      state = next_state\n",
    "    \n",
    "      if done:\n",
    "        break\n",
    "\n",
    "avg_reward = total_rewards / num_episodes_eval\n",
    "print(\"Average reward:\", avg_reward)\n",
    "env.close()\n",
    "\n",
    "env.reset()\n",
    "for episode in range(10):\n",
    "  state = env.reset()\n",
    "  step = 0\n",
    "  done = False\n",
    "  print(\"****************************************************\")\n",
    "  print(\"EPISODE \", episode)\n",
    "  for step in range(max_steps):\n",
    "    action = np.argmax(qtable[state,:])\n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "      if new_state == 15:\n",
    "        print(\"We reached our Goal \")\n",
    "      else:\n",
    "        print(\"We fell into a hole  \")\n",
    "        # We print the number of step it took.\n",
    "        print(\"Number of steps\", step)\n",
    "        break\n",
    "    state = new_state\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
